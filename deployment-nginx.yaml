---
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  nginx-dp
  namespace: default
  labels:
    app:  nginx-dp
spec:
  selector:
    matchLabels:
      app: nginx-dp
        #  replicas: 4 # колмчество подов не указываем. Поумолчанию будет 1, но автоскейлер их автоматически поднимет до 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0 # не допускаем ни одного не рабочего пода во время обновления.
    type: RollingUpdate
  template:
    metadata:
      labels:
        app:  nginx-dp
    spec:
      affinity: # Устанавливаем распределение подов на ноды трех зон. 4ый под будет распределен в случайную зону из трёх.
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
            - key: topology.kubernetes.io/zone
              operator: In
              values:
              - antarctica-east1
              - antarctica-west1
              - antarctica-west2
      containers:
      - name:  nginx-dp
        image:  nginx:1.23.1 # latest не стоит устанавливать, т.к. неизвестно что будет в более новой версии
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            cpu: 100m # Возможно стоило указать requests меньше лимита, т.к. это бы это возможно уменьшило счет за облако, но не уверен.
            memory: 128Mi # Ни разу не получал счет за облако, поэтому не знаю может ли это повлиять на биллинг
          limits: # поэтому решил оставить requests = limits, что бы QoS был Guaranteed
            cpu: 100m
            memory: 128Mi
        startupProbe: #таймаут по старту контейнера. 2 проверки периодичностью 5 секунд
          httpGet:
            path: /
            port: 80
          failureThreshold: 2
          periodSeconds: 5
        livenessProbe: # initialDelaySeconds не указываем, т.к. объявлен startupProbe
          tcpSocket:  # проверка жизни приложения во время работы. Простейшая проверка на прослушивание 80 порта.
            port: 80
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
          periodSeconds: 15
        readinessProbe: # initialDelaySeconds не указываем, т.к. объявлен startupProbe
          httpGet:  # проверка готовности приложения к работе. Данная проверка избыточна, т.к. существует startupProbe
            path: /
            port: 80
          timeoutSeconds: 2
          successThreshold: 1
          failureThreshold: 3
          periodSeconds: 10
        ports:
        - containerPort:  80
          name:  nginx-dp
      restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-80-port
  namespace: default
spec:
  selector:
    app: nginx-dp
  type: ClusterIP
  ports:
  - name: nginx
    protocol: TCP
    port: 80
    targetPort: 80
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-nginx-dp
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-dp
  minReplicas: 3 # из-за трех зон ставим минимальное количество подов в 3
  maxReplicas: 4 # 4 максимально, т.к. известно, что 4 пода переживают пиковую нагрузку
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70 # не дожидаемся пиковой нагрузки что бы запустить еще 1 под, а запускаем при утилизации лимитов cpu > 70%
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-dp-ingress
spec:
  defaultBackend:
    service:
      name: nginx-80-port
      port:
        number: 80
          # такой ингресс выглядит несколько избыточным, но без ингреса деплоймент выглядит неполным.
          # также для многозонального применение обычного ингреса мне кажется не корректным.
          # ингресс получит только 1 ip адрес и смысл многозональности пропадет, но какой ингерсс контрооллер применить в текущей ситуации не знаю.
